\documentclass[a4paper,12pt,oneside,final]{report}
\usepackage[pdftex]{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
\usepackage[titletoc]{appendix}
\titleformat{\chapter}[hang]{\bf\Huge}{\thechapter}{1cm}{}

\usepackage[colorlinks=true]{hyperref}
\hypersetup{urlcolor=blue,linkcolor=black,citecolor=black,colorlinks=true}
\bibliographystyle{plain}

\pagestyle{plain}
% -------------------- this stuff for code --------------------

\usepackage{anysize}
\marginsize{30mm}{30mm}{20mm}{20mm}

\newenvironment{formal}{%
  \def\FrameCommand{%
    \hspace{1pt}%
    {\color{blue}\vrule width 2pt}%
    {\color{formalshade}\vrule width 4pt}%
    \colorbox{formalshade}%
  }%
  \MakeFramed{\advance\hsize-\width\FrameRestore}%
  \noindent\hspace{-4.55pt}% disable indenting first paragraph
  \begin{adjustwidth}{}{7pt}%
  \vspace{2pt}\vspace{2pt}%
}
{%
  \vspace{2pt}\end{adjustwidth}\endMakeFramed%
}

\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}

\usepackage{color}
\usepackage{dsfont}
\usepackage[bitstream-charter]{mathdesign}
\usepackage[scaled]{helvet}
\usepackage{inconsolata}


\definecolor{colKeys}{rgb}{0,0,0.9} 
\definecolor{colIdentifier}{rgb}{0,0,0} 
\definecolor{colString}{rgb}{0.7,0,0} 
\definecolor{colComments}{rgb}{0,0.6,0} 
\usepackage{listings}
\lstset{
  stringstyle=\color{colString},
  keywordstyle=\color{colKeys},
  identifierstyle=\color{colIdentifier},
  commentstyle=\color{colComments},
  numbers=left,
  tabsize=4,
  frame=single,
  breaklines=true,
  basicstyle=\small\ttfamily,
  numberstyle=\tiny\ttfamily,
  framexleftmargin=0mm,
  xleftmargin=7mm,
  xrightmargin=7mm,
  frameround={tttt},
  captionpos=b
}

\usepackage{mathtools}
\usepackage{amsthm}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator*{\argmin}{ArgMin\ }
\DeclareMathOperator*{\argmax}{ArgMax\ }

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[usenames,dvipsnames]{xcolor}
\makeatletter
\DeclareRobustCommand{\em}{%
  \@nomath\em \if b\expandafter\@car\f@series\@nil
  \normalfont \else \bfseries \fi}
\makeatother

%% Headers and footers
\usepackage{fancyhdr}
\usepackage[section]{placeins}
\pagestyle{fancy}
\fancyhf{}
\addtolength{\headwidth}{30pt}
\addtolength{\headwidth}{30pt}
\renewcommand{\headrulewidth}{0.4pt} % thickness of the header line
\renewcommand{\footrulewidth}{0.4pt} % thickness of the footer line
\renewcommand{\chaptermark}[1]{\markboth{#1}{#1}} % chapter name
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}  % section name
\lhead[\fancyplain{}{\bf\thepage}]{\fancyplain{}{\bf\rightmark}} % display header
\rhead[\fancyplain{}{\bf\leftmark}]{\fancyplain{}{}} % display header
\fancyfoot[C]{\bf\thepage} % display footer (page number)
\fancyfoot[R]{\bf\today} % display footer (date)
\fancypagestyle{plain}{ 
	\fancyhead{} \renewcommand{\headrulewidth}{0pt}
}
\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{plain}\cleardoublepage}}

\usepackage[T1]{fontenc}
\usepackage{enumerate}
\usepackage{afterpage,lastpage,fancyhdr}
\usepackage[includeheadfoot,margin=2.5cm]{geometry}
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\makeatletter \def\thickhrulefill{\leavevmode \leaders \hrule height 1pt\hfill
\kern \z@} \renewcommand{\maketitle}{
    \begin{titlepage}
    \let\footnotesize\small \let\footnoterule\relax \parindent \z@ \reset@font
    \null\vfil
    \vspace{-20mm}
    \begin{center}
    {\small \scshape Imperial College London \\ Department of Computing}
    \end{center}
    \vspace{0.5cm}
	\begin{minipage}{\textwidth}
		\vspace{1cm}
		\noindent\rule[0ex]{\textwidth}{4pt} \\
		\flushright
		\center
		\@title
		\\ \vspace{4mm}
		\noindent\rule[0ex]{\textwidth}{4pt} \\
	\end{minipage}
	\vspace{1.5cm}
	\begin{minipage}{\textwidth}
		\flushright
		{\bfseries}
		\vspace{7mm}
		\flushleft
		\@author.\\
	\end{minipage}
	\vspace{0.5cm}
	\begin{center}
		\includegraphics[width=70mm,]{pictures/logo_imperial_college_london.png}
	\end{center}
	\vspace{\stretch{1}}
	\vspace{50mm}
		\flushleft
		{\bfseries}
		Module leader \& Lecturer: Dr Maja \textsc{Pantic}. \\
		{\small \scshape \@date }.
		\vspace{0.1cm}
		\rule{\linewidth}{.5pt}
  \end{titlepage}
  \setcounter{footnote}{1}
  \setcounter{page}{2}
}


\author{
    Sedef Ozlen (so512, s5), \\ 
    Paul Gribelyuk (pg1312, a5), \\
    Jean Kossaifi (jk712, a5), \\ 
    Romain Brault (rb812, a5)
}
\makeatother
\title{\Huge Machine Learning \\ Artificial Neural Network \\ Coursework 3}
\date{\today}


\usepackage{amsmath}
\begin{document}
\maketitle
\tableofcontents
\listoffigures

\chapter{Introduction}
\paragraph{}
More than only understanding how to use the MATLAB Neural Network Toolbox, the goal of this coursework is both to gain a global understanding of the inner functioning of a neural networks, and to be well aware of the variety of methods that can be used to classify the very dataset which was used while implementing decision trees.
During this coursework, we implemented both a single 6-way classifier and 6 different binary classifiers. The former directly yields a multi-class result whereas with the latter, we got 6 different results, corresponding to a "one-against all", which we combined to produce a single classification of the data.
\paragraph{}
All along the project, we used the MATLAB Neural Network Toolbox which provides a great deal of flexibility in specifying the exact network topology, in other words, the different parameters of the neural network, namely the performance measurement function, the learning rate (when computing weights via stochastic gradient descent), the transfer (or activation) function and the training function.

\chapter{Creating two types of neural networks}
\paragraph{}
First, we built two neural networks classifier to find a predictive model for emotions from Action Units (AU).
\section{Implementation of neural networks with the Matlab Toolbox}
\paragraph{} The function:
To be able to create easily both kind of network we have created a core function:
\begin{changemargin}{-5mm}{-5mm}
\begin{lstlisting}[language=Matlab, frame=single]
function [ network ] = buildNetwork( HiddenLayer, epochs, dataSplit, x, y, perf,  lr, tf, trainFn)
\end{lstlisting}
\end{changemargin}
Which build a single network according to argument parameters. 
\label{ch:build}
\begin{itemize}
\item {\bf \textit{HiddentLayer} } is a vector controlling the number of hidden layers of the neural network, as specified by the function \textit{feedforwardnet}.
\item {\bf \textit{epoch} } controls the number of iterations used by the training algorithm (set with the parameter trainFn) 
\item {\bf \textit{datasplit} } is a vector expressing the fraction of the database which must be used as training set, validation set and testing set. The sum of the elements of the vector must be equal to $1$.
\item {\bf \textit{$\bf x$, $\bf y$} } is the database ($x\in M_{n,m}([0,1])$ is the feature matrix, and $y\in M_{m,6}([0,1])$ the response vector where $n$ denotes the number of features and $m$ the number of elements in the database).
\item {\bf \textit{perf} } represents the performance measure to calculate the error.
\item {\bf \textit{lr} } is the learning rate for the training algorithm.
\item {\bf \textit{tf} } is the training function (also call activation function) for all neurones.
\item {\bf \textit{trainFun} } is the training algorithm.
\end{itemize}
We use this set-up throughout the assignment to externalize the creation of the network and to allow us to experiment with various parameter inputs when looking for an optimised network.
\paragraph{}
The response vector $y$ given in the matrix \textit{cleandata\_students.mat} belong to the set $[1,\hdots,6]^m$ and the feature array $x$ to $x\in M_{m,m}([0,1])$ which does not match the specification of teh function \textit{buildNetwork}. To convert these data in the appropriate form we use:
\begin{changemargin}{-5mm}{-5mm}
\begin{lstlisting}[language=Matlab, frame=single]
[x2, y2] = ANNdata(x, y);
\end{lstlisting}
\end{changemargin}
Henceforth, when they are not parameter of a function $x2$ and $y2$ denote the converted data whereas $x$ and $y$ denote the original data.
\paragraph{}
Then a single six-output neural network can be constructed by choosing the appropriate set of parameters and call this function on the whole database $(x2,y2)$. On the other hand to construct a single output neural network we only need to select the emotion we want to predict. This can be done easily by calling the function \textit{buildNetwork} on the database $(x2,y2(:,k))$ where $k\in[1,\hdots,6]$ is the emotion we want to predict.

\section{Testing the networks}
\paragraph{}
The prediction of the six-outputs neural network are obtained by calling \textit{sim} and can be converted from $M_{m,6}([0,1])$ back to $[1,\hdots,6]^m$ with the provided function \textit{NNout2labels}. The six single-output neural networks returns each, after calling \textit{sim} a prediction vector $pv\in \mathbb{R}^m$. As a result after calling \textit{sim} on each network we have a prediction array $pa\in M_{m,6}([0,1])$. As a result if the transfer function is an increasing function we can infer that each value on a line gives the strength of the element characterises an emotion. If the output space of the transfer function is $[0,1]$ we can interpret it as a probability (but it does not mean that the sum on each line is one). Hence a simple way to combine all networks is to choose the maximum value on each line and return the number of the neural network (its column number) who has done this prediction. After observing the source code of \textit{NNout2labels} we noticed that it was exactly what this function was doing. Consequently to combine the results of the six single-output neural networks, we concatenates all prediction columns in a big array and called \textit{NNout2labels} on the array, exactly like we do for the single six-outputs neural network. The function:
\begin{changemargin}{-5mm}{-5mm}
\begin{lstlisting}[language=Matlab, frame=single]
function predictions = testANN( net, examples )
\end{lstlisting}
\end{changemargin}
determines whether net is a six single-output or a single six-outputs neural network and compute the predictions of the vector \textit{examples}.


\chapter{Optimised Networks}
\paragraph{}
As we have seen section \ref{ch:build} building neural networks require to choose a lot of parameters which need to be tuned to obtain valid results. After explaining how we have found the optimal parameters we discuss the results and performances obtained.
\section{Optimizing the topology and parameters of the networks}
\subsection{Error measure}
Among four error measures possible, we have selected the Mean Square Error (MSE). The possible functions where:
\begin{itemize}
\item { \bf Sum Absolute Error (SAE) and Sum Square Error (SSE):} these functions both sum the error (absolute value or square of the difference between predicted and actual value) committed over all test points. Since they are not normalized their value can increase artificially with the number of examples, therefore they are not suitable to compare performances over testing set of different size; and we prefer use their normalised version.
\item { \bf Mean Absolute Error (MAE):} The optimal solution to the MAE is the median. In other words, this error measure penalises important misses in the same proportion than small misses. With this function we favor general accuracy with possible misses.
\item { \bf Mean Square Error (MSE):} The optimal solution to the MSE is the mean. In other words, this error measure penalises in a bigger proportion important misses than small misses. This functions favors a less overall accuracy for a fewer number of misses.
\end{itemize}
Since we are predicting emotions, there is no false predictions that are worst than an other. Hence MSE seems more appropriate than MAE because the proportions of mis predictions is likely to be less important.
\subsection{Optimal topology}
\paragraph{}
The theory of neural tells us that with a two layers neural network, we are able to approximate any function with any wanted precision, whereas one layer neural networks doesn't allow us to deal with non-linearly separable problems. For this reason we decided to focus only on two layers neural networks.
\subsection{Optimal Parameters}
\label{ch:opset}
\paragraph{}
To tune the parameters: \textit{HiddenLayer} (number of neurons, depends on the topology), \textit{lr} (learning rate), \textit{tf} (transfer function), \textit{trainFn} (training algorithm) we used a search grid. It is an 4 dimensions array where each cell correspond to a possible combination of \textit{HiddenLayer}, \textit{lr}, \textit{tf}, \textit{trainFn}. We train a neural network for each cell of the array with 67\% of the original database and test it on a validation set with the 37\% remaining elements of the database. Hence the optimal parameter set correspond to the cell which has minimise the error on the validation set.
\paragraph{}
We have tested a variety of potential inputs for the \textit{activation function}, \textit{number of neurons}, \textit{training function}, and learning rate.  After some research, the following activation functions remained the most directly applicable ones to test:
\begin{itemize}
\item \textit{tansig} : $\text{tansig}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
\item \textit{tansig} : $\text{hardlim}(x) = 1$ for $x>0$ and $\text{hardlim}(x) = 0$ for $x \leq 0$.
\item \textit{logsig} : $\text{logsig}(x) = (1 + e^{-x})^{-1}$
\item\textit{purelin} : $\text{purelin}(x) = x$
\item \textit{radbasn} : $\text{radbasn}(x) = e^{-(x)^2}$
\item \textit{softmax} : $\text{softmax}_i(q) = \frac{e^{q_i}}{\sum_1^n e^{q_j}}$
\end{itemize}
\paragraph{}
For example we have discarded the \textit{compet} transfer function because it only select one attribute. Furthermore, we have narrowed down the training functions used to approach the solution to the following:
\begin{itemize}
\item \textit{trainlm} : Levenberg-Marquardt method which interpolates between Gauss-Neuton and gradient descent methods.
\item \textit{trainbfg} : Broyden-Fletcher-Goldfarb-Shanno method which is similar to Newton's method
\item \textit{trainbr} : Bayesian Regularization method is an extension to \textit{trainlm} with expected better out-of-sample performance due to regularization or weights
\item\textit{traincgb} : Conjugate backpropagation technique
\item \textit{traincgf} : Another conjugate backpropagation with a different update method  
\item \textit{traincgp} : Yet another conjugate backpropagation with a different update method   
\item \textit{traingdm} : Gradient descent with momentum changing according to the convergence path taken
\item \textit{traingdx} : Gradient descent with momentum and an adaptive learning rate
\item \textit{trainscg} : Scaled conjugate gradient backpropagation
\end{itemize}
\paragraph{}
The parameter $epoch$ which represent the number of iteration of the training algorithm is not really a parameter to tune, since the higher its value is, the better will be the performances of the neural network. However setting a high value to $epoch$ leads to important computation time. In our case $epoch=20$ is a good trade-off between computation time and performances.

{\color{red} Add optimal parameter set found}

\section{Train a network}
{\color{red} entire dataset to train the network ?}

\section{Performing 10 fold cross-validation}
{\color{red} confusions matrix ?}

\paragraph{}
Since we have determined an optimal parameter set on a validation set, problems can arise if this validation set is used as testing set. Indeed, while selecting the optimal parameter set, the neural network could have over fit to minimise the error on the validation set; and the result of the cross-validation might be biased.
\paragraph{}
To avoid the introduction of bias, we should find the optimal parameters inside the cross-validation with a cross validation. This process is called double cross validation and works the following way:
\begin{algorithm}[H]
\caption{Double cross-validation}
\label{al:dcv}
\begin{algorithmic}[1]
\STATE Randomly split respectively $(x, y)$ into $n$ subsamples $(x^{(1)}, y^{(1)}),\hdots,(x^{(n)}, y^{(n)})$
\FOR{$k=1\hdots n$} 
    \STATE 1) Create the training base $(x2,y2)=(x, y)\setminus(x^{(k)}, y^{(k)})$.
    \STATE 2) On the training base $(x2,y2)$ find the optimal parameter set with a grid search by measuring the error with a cross validation.
    \STATE 3) Evaluate the error $E_k$ on the testing set $x^{(k)}, y^{(k)}$
\ENDFOR
\STATE {\bf return } the average error of $E_k$.
\end{algorithmic}
\end{algorithm}
\paragraph{}
Notice that step 2) we select the optimal parameter set with a cross validation. To save computation time we could simply divide the training set $(x2,y2)$ into a sub-training set $(x2,y2)_{st}$ with $67\%$ of $(x2,y2)$, a validation set $(x2,y2)_{sv}$ with the $33\%$ remaining and select the optimal parameter set exactly the same way than in section \ref{ch:opset}.

\section{Plotting performance measure}
{\color{red} Performances}

\section{Single six-outputs VS. six single-output neural network}
\paragraph{}
The six single-output neural network performs better than the single six-outputs neural network. The single six-outputs has the advantage to work in one block so it can infer a way to classify an emotion with information from the other emotions. On the other hand each network of six single-output only have the information for one emotion. Consequently this network has better results on small databases but its results highly depends on the way of combining the six outputs.
\chapter{Conclusion}
\paragraph{}
Eventually, an important lesson we learned is that although Neural Networks is powerful, the search for an optimal network is extremely computationally intensive.  Different parameters yield different results which have to be tuned differently according to the context. Parameters also have to be chosen together because choosing parameters one after another could lead to a local minimum and not to the global minimum. The advent of back propagation in conjunction with stochastic gradient descent in the 1970s, when optimizing node weights, accelerated the study of neural networks for real-world applications, spawning the use of a variety of \emph{performance measures}, \emph{activation functions} (a.k.a. transfer functions), and \emph{training functions}.  In this report, we will show our results for the classification rate of the sample data for different network topologies, activation and transfer functions, as well as the effects of a variable learning rate. 

\bibliographystyle{alpha}
\bibliography{biblio.bib}

\begin{appendices}

\end{appendices}

\end{document}  

%-----------------------------------------------------------------------
% Beginning of amsart.template
%-----------------------------------------------------------------------
%
%     AMS-LaTeX v.2 template for use with amsart
%
%     Remove any commented or uncommented macros you do not use.

\documentclass{amsart}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\numberwithin{equation}{section}

\begin{document}

\title{Machine Learning}

%    Remove any unused author tags.

%    author one information
%\author{}
%\address{}
%\curraddr{}
%\email{}
%\thanks{}
\date{}

\maketitle

\section{Concept Learning}
Target concept: girls who like Simon \\
Target function: \\
data: girls described by: hair, body, likes Simon, pose, smile, smart \\

Idea is to find best hypothesis to minimize \emph{error rate}\\
Finite search space since data items are finite

general-to-specific ordering:
- h1 \emph{precedes} h2 off for all data, h1(data) gives positive classification <- h2(data) gives positive classification so
$$
h1 >_{g} h2
$$

- h2 and h2 have equal generality if exists data item if h1(d) = I -> h2(d)= I or h2(d) = I -> h1(d) = I, then
$$
h1 =_{g} h2
$$
If two hypotheses have the same number of defined attributes, they are of the same generality.
\\
\\
\emph{Find-S Algorithm}\\
\begin{itemize}
\item[-] Initialize hypothesis $h$ to most specific hypothesis
\item[-] iterate over all data $d\in D$: iterate over all attributes in $h$:
		if the attribute is not satisfied by $d$ replace the attribute s.t. $h'>_{g}h, h\leftarrow h'$
\end{itemize}
Lots of limitations of this algorithms as could output only one of many equally valid hypotheses.
\\
\\
\emph{Candidate Elimination Algorithms}\\
\begin{itemize}
\item general-to-specific ordering of hypotheses
\item two-sided approach to converging to solution
\item specific boundary (initialize all attributes) works with positive examples and eliminates those that don't fit
\item general boundary (let all attributes vary) works with negative examples and eliminates those that don't' fit
\end{itemize}

\section{Decision Trees}
Used for discrete approximation: helpful when \emph{if-then} classification is needed.  Task scheduling is a typical problem solved by decision trees.  To lean using decision trees, use greedy search through space of possible solutions.  Algorithm:
\begin{description}
\item[1] perform statistical test to determine how well the attribute classifies training data
\item[2] best attribute forms root of tree
\item[3] descendant node in each branch will be determined by attributes of the node; split data according to those attributes and continue
\end{description}
ID3 algorithm uses \emph{information gain}:
$$
E(S) \equiv -\sum_v p_v\ln(p_v)
$$
where $v\in \{1\ldots n\}$ and then
$$
IG(S, A) = E(S) - \sum_{v\in values(A)}\left(\frac{|S|}{|S_v|}\right) E(S_v)
$$
Iteratively, we take the value of the attribute that has the best explanatory power (information gain) and make that part of the decision tree deterministic.  The other values will be classified again with subsets of the data set, and the process repeats itself. 
















\end{document}
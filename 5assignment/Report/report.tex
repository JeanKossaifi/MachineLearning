\documentclass[a4paper,12pt,oneside,final]{report}
\usepackage[pdftex]{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
\usepackage[titletoc]{appendix}
\titleformat{\chapter}[hang]{\bf\Huge}{\thechapter}{1cm}{}

\usepackage[colorlinks=true]{hyperref}
\hypersetup{urlcolor=blue,linkcolor=black,citecolor=black,colorlinks=true}
\bibliographystyle{plain}

\pagestyle{plain}
% -------------------- this stuff for code --------------------

\usepackage{anysize}
\marginsize{30mm}{30mm}{20mm}{20mm}

\newenvironment{formal}{%
  \def\FrameCommand{%
    \hspace{1pt}%
    {\color{blue}\vrule width 2pt}%
    {\color{formalshade}\vrule width 4pt}%
    \colorbox{formalshade}%
  }%
  \MakeFramed{\advance\hsize-\width\FrameRestore}%
  \noindent\hspace{-4.55pt}% disable indenting first paragraph
  \begin{adjustwidth}{}{7pt}%
  \vspace{2pt}\vspace{2pt}%
}
{%
  \vspace{2pt}\end{adjustwidth}\endMakeFramed%
}

\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}

\usepackage{color}
\usepackage{dsfont}
\usepackage[bitstream-charter]{mathdesign}
\usepackage[scaled]{helvet}
\usepackage{inconsolata}


\definecolor{colKeys}{rgb}{0,0,0.9} 
\definecolor{colIdentifier}{rgb}{0,0,0} 
\definecolor{colString}{rgb}{0.7,0,0} 
\definecolor{colComments}{rgb}{0,0.6,0} 
\usepackage{listings}
\lstset{
  stringstyle=\color{colString},
  keywordstyle=\color{colKeys},
  identifierstyle=\color{colIdentifier},
  commentstyle=\color{colComments},
  numbers=left,
  tabsize=4,
  frame=single,
  breaklines=true,
  basicstyle=\small\ttfamily,
  numberstyle=\tiny\ttfamily,
  framexleftmargin=0mm,
  xleftmargin=7mm,
  xrightmargin=7mm,
  frameround={tttt},
  captionpos=b
}

\usepackage{mathtools}
\usepackage{amsthm}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator*{\argmin}{ArgMin\ }
\DeclareMathOperator*{\argmax}{ArgMax\ }

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[usenames,dvipsnames]{xcolor}
\makeatletter
\DeclareRobustCommand{\em}{%
  \@nomath\em \if b\expandafter\@car\f@series\@nil
  \normalfont \else \bfseries \fi}
\makeatother

%% Headers and footers
\usepackage{fancyhdr}
\usepackage[section]{placeins}
\pagestyle{fancy}
\fancyhf{}
\addtolength{\headwidth}{30pt}
\addtolength{\headwidth}{30pt}
\renewcommand{\headrulewidth}{0.4pt} % thickness of the header line
\renewcommand{\footrulewidth}{0.4pt} % thickness of the footer line
\renewcommand{\chaptermark}[1]{\markboth{#1}{#1}} % chapter name
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}  % section name
\lhead[\fancyplain{}{\bf\thepage}]{\fancyplain{}{\bf\rightmark}} % display header
\rhead[\fancyplain{}{\bf\leftmark}]{\fancyplain{}{}} % display header
\fancyfoot[C]{\bf\thepage} % display footer (page number)
\fancyfoot[R]{\bf\today} % display footer (date)
\fancypagestyle{plain}{ 
	\fancyhead{} \renewcommand{\headrulewidth}{0pt}
}
\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{plain}\cleardoublepage}}

\usepackage[T1]{fontenc}
\usepackage{enumerate}
\usepackage{afterpage,lastpage,fancyhdr}
\usepackage[includeheadfoot,margin=2.5cm]{geometry}
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\makeatletter \def\thickhrulefill{\leavevmode \leaders \hrule height 1pt\hfill
\kern \z@} \renewcommand{\maketitle}{
    \begin{titlepage}
    \let\footnotesize\small \let\footnoterule\relax \parindent \z@ \reset@font
    \null\vfil
    \vspace{-20mm}
    \begin{center}
    {\small \scshape Imperial College London \\ Department of Computing}
    \end{center}
    \vspace{0.5cm}
	\begin{minipage}{\textwidth}
		\vspace{1cm}
		\noindent\rule[0ex]{\textwidth}{4pt} \\
		\flushright
		\center
		\@title
		\\ \vspace{4mm}
		\noindent\rule[0ex]{\textwidth}{4pt} \\
	\end{minipage}
	\vspace{1.5cm}
	\begin{minipage}{\textwidth}
		\flushright
		{\bfseries}
		\vspace{7mm}
		\flushleft
		\@author.\\
	\end{minipage}
	\vspace{0.5cm}
	\begin{center}
		\includegraphics[width=70mm,]{pictures/logo_imperial_college_london.png}
	\end{center}
	\vspace{\stretch{1}}
	\vspace{50mm}
		\flushleft
		{\bfseries}
		Module leader \& Lecturer: Dr Maja \textsc{Pantic}. \\
		{\small \scshape \@date }.
		\vspace{0.1cm}
		\rule{\linewidth}{.5pt}
  \end{titlepage}
  \setcounter{footnote}{1}
  \setcounter{page}{2}
}


\author{
    Sedef Ozlen (so512, s5), \\ 
    Paul Gribelyuk (pg1312, a5), \\
    Jean Kossaifi (jk712, a5), \\ 
    Romain Brault (rb812, a5)
}
\makeatother
\title{\Huge Machine Learning \\ T-test \\ Coursework 5}
\date{\today}


\usepackage{amsmath}
\begin{document}
\maketitle
\tableofcontents
\listoffigures


\chapter{Results and Questions}
We loaded our three algorithms, the Decision Trees, the Neural Networks, and the CBR to determine whether any there is any statistically significan difference between their performance.  Our script which runs all the t-tests is saved in $Tests.m$ and is attached with this submission.
\begin{enumerate}
\item The CBR algorithm (K Nearest Neighbors) had the best $F_1$ measure.  When we ran the t-test script we created
Here are the detailed F1 scores for each classifier for each of the folds.

\begin{tabular}{llll}

Fold   &     Decision Tree                          &    Neural network                          &   Case-base Reasoning                        \\
Fold 1 &  84.62  80.00  77.78  87.80  75.00  80.00  &  75.00  81.82  90.91  92.31  75.00  92.68  &  76.92  80.00  100.00  95.24  83.33  95.00   \\ 
Fold 2 &  64.52  71.43  40.00  87.80  28.57  86.36  &  73.33  86.49  77.78  95.24  72.00  95.24  &  88.00  92.68  73.68  100.00  69.23  97.56   \\ 
Fold 3 &  81.48  87.80  72.73  90.48  66.67  85.71  &  74.07  85.00  86.96  90.48  66.67  94.74  &  96.00  92.68  86.96  93.33  69.57  91.89   \\ 
Fold 4 &  71.43  78.95  78.26  92.68  71.43  77.78  &  78.57  76.47  85.71  92.68  73.33  85.00  &  92.86  89.47  81.82  97.67  86.96  90.00   \\ 
Fold 5 &  69.57  80.95  70.00  95.24  66.67  90.00  &  76.92  78.05  80.00  92.68  72.73  97.44  &  72.00  80.95  90.00  95.24  84.62  97.44   \\ 
Fold 6 &  52.63  79.07  70.00  95.45  69.23  76.19  &  63.64  81.08  73.68  93.33  85.71  93.02  &  69.23  86.49  84.21  95.45  84.62  95.24   \\ 
Fold 7 &  81.48  82.35  81.82  93.02  66.67  92.68  &  76.92  82.05  85.71  92.68  80.00  90.48  &  81.48  77.27  95.65  90.48  63.16  97.44   \\ 
Fold 8 &  78.57  80.00  78.26  80.95  60.00  83.33  &  76.92  66.67  69.57  85.00  61.54  81.08  & 82.76  75.56  75.00  92.68  55.56  91.89   \\ 
Fold 9 &  88.89  69.77  85.71  95.00  56.00  84.21  &  76.92  84.21  85.71  95.24  72.00  95.24  &  83.33  81.82  86.96  95.24  76.19  95.00   \\ 
Fold 10 &  75.86  72.00  78.95  84.62  58.82  77.97  &  52.17  74.51  90.00  92.86  62.86  91.23  &  81.48  83.64  85.71  89.29  64.29  92.59   \\ 

\end{tabular}


***** General results ******
Decision Tree
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 

Neural Network
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 

Case-base reasoning
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 


\item To adjust the significance level to take into account the fact that we are doing multiple comparisons, we used $\alpha = 0.05 / k$, where $k = 3$ since we are making 3 comparisons, as we are comparing the 3 models.
\item We used the paired t-test which considers the possiblity that the two sets of data are not independent.  We believe this to be true because althoough they are different models, which were used, the data on which these models trained was the same, thus, some dependence is likely present.
\item We performed the t-test on the classification rate rather than the $F_1$ measure because 
\end{enumerate}

\bibliographystyle{alpha}
\bibliography{biblio.bib}

\begin{appendices}

\end{appendices}

\end{document}  


\documentclass[a4paper,12pt,oneside,final]{report}
\usepackage[pdftex]{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
\usepackage[titletoc]{appendix}
\titleformat{\chapter}[hang]{\bf\Huge}{\thechapter}{1cm}{}

\usepackage[colorlinks=true]{hyperref}
\hypersetup{urlcolor=blue,linkcolor=black,citecolor=black,colorlinks=true}
\bibliographystyle{plain}

\pagestyle{plain}
% -------------------- this stuff for code --------------------

\usepackage{anysize}
\marginsize{30mm}{30mm}{20mm}{20mm}

\newenvironment{formal}{%
  \def\FrameCommand{%
    \hspace{1pt}%
    {\color{blue}\vrule width 2pt}%
    {\color{formalshade}\vrule width 4pt}%
    \colorbox{formalshade}%
  }%
  \MakeFramed{\advance\hsize-\width\FrameRestore}%
  \noindent\hspace{-4.55pt}% disable indenting first paragraph
  \begin{adjustwidth}{}{7pt}%
  \vspace{2pt}\vspace{2pt}%
}
{%
  \vspace{2pt}\end{adjustwidth}\endMakeFramed%
}

\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}

\usepackage{color}
\usepackage{dsfont}
\usepackage[bitstream-charter]{mathdesign}
\usepackage[scaled]{helvet}
\usepackage{inconsolata}


\definecolor{colKeys}{rgb}{0,0,0.9} 
\definecolor{colIdentifier}{rgb}{0,0,0} 
\definecolor{colString}{rgb}{0.7,0,0} 
\definecolor{colComments}{rgb}{0,0.6,0} 
\usepackage{listings}
\lstset{
  stringstyle=\color{colString},
  keywordstyle=\color{colKeys},
  identifierstyle=\color{colIdentifier},
  commentstyle=\color{colComments},
  numbers=left,
  tabsize=4,
  frame=single,
  breaklines=true,
  basicstyle=\small\ttfamily,
  numberstyle=\tiny\ttfamily,
  framexleftmargin=0mm,
  xleftmargin=7mm,
  xrightmargin=7mm,
  frameround={tttt},
  captionpos=b
}

\usepackage{mathtools}
\usepackage{amsthm}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\DeclareMathOperator*{\argmin}{ArgMin\ }
\DeclareMathOperator*{\argmax}{ArgMax\ }

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[usenames,dvipsnames]{xcolor}
\makeatletter
\DeclareRobustCommand{\em}{%
  \@nomath\em \if b\expandafter\@car\f@series\@nil
  \normalfont \else \bfseries \fi}
\makeatother

%% Headers and footers
\usepackage{fancyhdr}
\usepackage[section]{placeins}
\pagestyle{fancy}
\fancyhf{}
\addtolength{\headwidth}{30pt}
\addtolength{\headwidth}{30pt}
\renewcommand{\headrulewidth}{0.4pt} % thickness of the header line
\renewcommand{\footrulewidth}{0.4pt} % thickness of the footer line
\renewcommand{\chaptermark}[1]{\markboth{#1}{#1}} % chapter name
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}  % section name
\lhead[\fancyplain{}{\bf\thepage}]{\fancyplain{}{\bf\rightmark}} % display header
\rhead[\fancyplain{}{\bf\leftmark}]{\fancyplain{}{}} % display header
\fancyfoot[C]{\bf\thepage} % display footer (page number)
\fancyfoot[R]{\bf\today} % display footer (date)
\fancypagestyle{plain}{ 
	\fancyhead{} \renewcommand{\headrulewidth}{0pt}
}
\newcommand{\clearemptydoublepage}{\newpage{\pagestyle{plain}\cleardoublepage}}

\usepackage[T1]{fontenc}
\usepackage{enumerate}
\usepackage{afterpage,lastpage,fancyhdr}
\usepackage[includeheadfoot,margin=2.5cm]{geometry}
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\makeatletter \def\thickhrulefill{\leavevmode \leaders \hrule height 1pt\hfill
\kern \z@} \renewcommand{\maketitle}{
    \begin{titlepage}
    \let\footnotesize\small \let\footnoterule\relax \parindent \z@ \reset@font
    \null\vfil
    \vspace{-20mm}
    \begin{center}
    {\small \scshape Imperial College London \\ Department of Computing}
    \end{center}
    \vspace{0.5cm}
	\begin{minipage}{\textwidth}
		\vspace{1cm}
		\noindent\rule[0ex]{\textwidth}{4pt} \\
		\flushright
		\center
		\@title
		\\ \vspace{4mm}
		\noindent\rule[0ex]{\textwidth}{4pt} \\
	\end{minipage}
	\vspace{1.5cm}
	\begin{minipage}{\textwidth}
		\flushright
		{\bfseries}
		\vspace{7mm}
		\flushleft
		\@author.\\
	\end{minipage}
	\vspace{0.5cm}
	\begin{center}
		\includegraphics[width=70mm,]{pictures/logo_imperial_college_london.png}
	\end{center}
	\vspace{\stretch{1}}
	\vspace{50mm}
		\flushleft
		{\bfseries}
		Module leader \& Lecturer: Dr Maja \textsc{Pantic}. \\
		{\small \scshape \@date }.
		\vspace{0.1cm}
		\rule{\linewidth}{.5pt}
  \end{titlepage}
  \setcounter{footnote}{1}
  \setcounter{page}{2}
}


\author{
    Sedef Ozlen (so512, s5), \\ 
    Paul Gribelyuk (pg1312, a5), \\
    Jean Kossaifi (jk712, a5), \\ 
    Romain Brault (rb812, a5)
}
\makeatother
\title{\Huge Machine Learning \\ T-test \\ Coursework 5}
\date{\today}


\usepackage{amsmath}
\begin{document}
\maketitle
%\tableofcontents
%\listoffigures


\chapter{Results and Questions}
We loaded our three algorithms, the Decision Trees, the Neural Networks, and the CBR to determine whether any there is any statistically significan difference between their performance.  Our script which runs all the t-tests is saved in $Tests.m$ and is attached with this submission.
\begin{enumerate}
\item The CBR algorithm (K Nearest Neighbors) had the best $F_1$ measure.  When we ran the t-test script we created
Here are the detailed F1 scores for each classifier for each of the folds.

\begin{itemize}
\item On clean data:
\begin{table} [H]
\begin{center}
\begin{tabular}{l|llllll}
       &  EM 1 & EM 2 & EM 3 & EM 4 & EM 5 & EM 6    \\ \hline
Fold 1 &  84.62 & 80.00 & 77.78 & 87.80 & 75.00 & 80.00    \\ 
Fold 2 &  64.52 & 71.43 & 40.00 & 87.80 & 28.57 & 86.36    \\ 
Fold 3 &  81.48 & 87.80 & 72.73 & 90.48 & 66.67 & 85.71   \\ 
Fold 4 &  71.43 & 78.95 & 78.26 & 92.68 & 71.43 & 77.78   \\ 
Fold 5 &  69.57 & 80.95 & 70.00 & 95.24 & 66.67 & 90.00   \\ 
Fold 6 &  52.63 & 79.07 & 70.00 & 95.45 & 69.23 & 76.19   \\ 
Fold 7 &  81.48 & 82.35 & 81.82 & 93.02 & 66.67 & 92.68   \\ 
Fold 8 &  78.57 & 80.00 & 78.26 & 80.95 & 60.00 & 83.33  \\ 
Fold 9 &  88.89 & 69.77 & 85.71 & 95.00 & 56.00 & 84.21   \\ 
Fold 10 &  75.86 & 72.00 & 78.95 & 84.62 & 58.82 & 77.97   \\ 
\end{tabular}
\caption{ Decision Tree  }
\end{center}
\end{table}

\begin{table} [H]
\begin{center}
\begin{tabular}{l|llllll}
       &  EM 1 & EM 2 & EM 3 & EM 4 & EM 5 & EM 6    \\ \hline
Fold 1 &  75.00 & 81.82 & 90.91 & 92.31 & 75.00 & 92.68    \\ 
Fold 2 &  73.33 & 86.49 & 77.78 & 95.24 & 72.00 & 95.24    \\ 
Fold 3 &  74.07 & 85.00 & 86.96 & 90.48 & 66.67 & 94.74   \\ 
Fold 4 &  78.57 & 76.47 & 85.71 & 92.68 & 73.33 & 85.00   \\ 
Fold 5 &  76.92 & 78.05 & 80.00 & 92.68 & 72.73 & 97.44   \\ 
Fold 6 &  63.64 & 81.08 & 73.68 & 93.33 & 85.71 & 93.02   \\ 
Fold 7 &  76.92 & 82.05 & 85.71 & 92.68 & 80.00 & 90.48   \\ 
Fold 8 &  76.92 & 66.67 & 69.57 & 85.00 & 61.54 & 81.08  \\ 
Fold 9 &  76.92 & 84.21 & 85.71 & 95.24 & 72.00 & 95.24   \\ 
Fold 10 &  52.17 & 74.51 & 90.00 & 92.86 & 62.86 & 91.23   \\ 
\end{tabular}
\caption{ Neural Network}
\end{center}
\end{table}


\begin{table} [H]
\begin{center}
\begin{tabular}{l|llllll}
       &  EM 1 & EM 2 & EM 3 & EM 4 & EM 5 & EM 6    \\ \hline
Fold 1 &  76.92 & 80.00 & 100.00 & 95.24 & 83.33 & 95.00   \\ 
Fold 2 &  88.00 & 92.68 & 73.68 & 100.00 & 69.23 & 97.56   \\ 
Fold 3 &  96.00 & 92.68 & 86.96 & 93.33 & 69.57 & 91.89   \\ 
Fold 4 &  92.86 & 89.47 & 81.82 & 97.67 & 86.96 & 90.00   \\ 
Fold 5 &  72.00 & 80.95 & 90.00 & 95.24 & 84.62 & 97.44   \\ 
Fold 6 &  69.23 & 86.49 & 84.21 & 95.45 & 84.62 & 95.24   \\ 
Fold 7 &  81.48 & 77.27 & 95.65 & 90.48 & 63.16 & 97.44   \\ 
Fold 8 & 82.76 & 75.56 & 75.00 & 92.68 & 55.56 & 91.89   \\ 
Fold 9 &  83.33 & 81.82 & 86.96 & 95.24 & 76.19 & 95.00   \\ 
Fold 10 &  81.48 & 83.64 & 85.71 & 89.29 & 64.29 & 92.59   \\ 
\end{tabular}
\caption{ Case Base Reasoning}
\end{center}
\end{table}

\item On noisy data:

\begin{table} [H]
\begin{center}
\begin{tabular}{l|llllll}
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
80.87 63.11 80.81 78.42 78.74 73.76 83.00 76.85 79.93 74.70 
\end{tabular}
\caption{ Decision Tree  }
\end{center}
\end{table}


Neural Network
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 
84.62 83.35 82.99 81.96 82.97 81.75 84.64 73.46 84.89 77.27 

Case-base reasoning
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
88.42  86.86  88.40  89.80  86.71  85.87  84.25  78.91  86.42  82.83 
\end{itemize}

\item To adjust the significance level to take into account the fact that we are doing multiple comparisons, we used $\alpha = 0.05 / k$, where $k = 3$ since we are making 3 comparisons, as we are comparing the 3 models.

\item We used the paired t-test which considers the possiblity that the two sets of data are not independent.  We believe this to be true because althoough they are different models, which were used, the data on which these models trained was the same, thus, some dependence is likely present.

\item F-measure can also be used in t-tests instead of classification rates, however, in our case, we want to learn how good our classifier classifies for each distinct class. In that case, we also care about the negative values too, and since f-measure does not take into account true negatives, it is not the most appropriate measure in our case. In addition we performed the t-test on the classification rate rather than the $F_1$ measure because the t-test assumes a normal distribution for the value for which we are testing the hypothesis (e.g. that $Z_{F_1} = |\mu_{NN}(F_1) - \mu_{CBR}(F_1)|$ or $Z_{CR} = |\mu_{NN}(classRate) - \mu_{CBR}(classRate)|$).  If the noise component of the model's predictions are normal, then the nonlinear combination of these errors is not normally distributed (unless the number of true positives is the same as the number of true positives).  Furthermore, the $F_1$ measure does not take true negatives into account as seen below:
$$
F_1 = 2\frac{tp + tn}{2\cdot tp + fn + fp}
$$
whereas the classification rate can be considered to be:
$$
classRate = \frac{tp + tn}{tp + tn + fp + fn}
$$
\item The higher the number of folds is, the more training data and consequently, the less test data we get per fold. If we use fewer folds, we end up with a higher amount of test data per fold, which would make our error rate more reliable, however, we will also have fewer training data. Fewer training data is especially more dangerous for eager learning algorithms, because it may lead to a deficient global approximation. Since lazy learning algorithms are able to adapt themselves for each new example they encounter, fewer training data would cause less trouble.

\item If we wanted to add new emotions, CBR is more suitable because it is a lazy learning algorithm.  If we add an adapt function for the solution of the nearest neighbour, then, easily we would be able to add a new emotion without requiring much change to the existing CBR system.
    
However, Neural Networks and Decision Trees are eager learning algorithms and make global approximations on the training data and use that approximation for the new examples. Therefore, since the global approximation is made on the training data set, and the same approximation is used for each new example, to add a new emotion we have to make a new global approximation.  For both decision trees and neural networks, adding a new emotion would mean to change the structure of the decision tree or the neural network.
\end{enumerate}

\bibliographystyle{alpha}
\bibliography{biblio.bib}

\begin{appendices}

\end{appendices}

\end{document}  


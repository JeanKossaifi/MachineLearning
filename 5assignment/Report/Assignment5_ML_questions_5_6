
5-) The higher the number of folds is, the more training data and consequently, the less test data we get per fold. 
    The increase in the training data is dangerous because it may lead to overfitting at some point,
    and since we have fewer test data, it may be hard to detect it. If we use fewer folds, we end up 
    with a higher amount of test data per fold, which would make our error rate more reliable, however, we 
    will also have fewer training data. Fewer training data is especially more dangerous for eager learning
    algorithms, because it may lead to a deficient global approximation. Since lazy learning algorithms are able to adapt
    themselves for each new example they encounter, fewer training data would cause less trouble. 

6-) If we wanted to add new emotions, CBR is more suitable because it is a lazy learning algorithm. 
    If we add an adapt function for the solution of the nearest neighbour, then, easily we will be able to add
    a new emotion without requiring much change to the existing CBR system.
    
    However, Neural Networks and Decision Trees are eager learning algorithms and make a global approximation on the training data anad
    use that approximation for the new examples. Therefore, since the global approximation is made on the training data set, 
    and the same approximation is used for each new example, to add a new emotion we have to make a new global approximation.
    For both decision trees and neural networks, adding a new emotion would mean to change the structure of the decision tree or the
    neural network.
 
